{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Transformer Implementation in PyTorch\n",
                "\n",
                "This notebook implements a Transformer model from scratch for English-to-Bengali translation, following the architecture from \"Attention Is All You Need\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.nn.utils.rnn import pad_sequence\n",
                "import csv"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Self Attention\n",
                "The `SelfAttention` mechanism splits the embedding into multiple heads to allow the model to attend to different parts of the sequence simultaneously. It computes Query, Key, and Value matrices and calculates attention scores using scaled dot-product attention."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SelfAttention(nn.Module):\n",
                "    def __init__(self, embed_size, heads, dropout):\n",
                "        # According to paper:\n",
                "        # embed_size(d_model) = 512\n",
                "        # heads(h) = 8\n",
                "        # So, head_dim(d_model/h) = 512/8 = 64\n",
                "        super(SelfAttention, self).__init__()\n",
                "        self.embed_size = embed_size\n",
                "        self.heads = heads\n",
                "        self.head_dim = embed_size // heads\n",
                "\n",
                "        # Checking if (d_model/h)*h = d_model\n",
                "        assert self.head_dim * heads == embed_size\n",
                "\n",
                "        self.values = nn.Linear(embed_size, embed_size)\n",
                "        self.keys = nn.Linear(embed_size, embed_size)\n",
                "        self.queries = nn.Linear(embed_size, embed_size)\n",
                "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
                "\n",
                "        self.attn_dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, values, keys, queries, mask):\n",
                "        # N stores the no. of rows in Q vector\n",
                "        N = queries.shape[0]\n",
                "\n",
                "        # These variables stores the no. of columns of V, K, Q\n",
                "        value_len, key_len, query_len = (\n",
                "            values.shape[1],\n",
                "            keys.shape[1],\n",
                "            queries.shape[1],\n",
                "        )\n",
                "\n",
                "        values = self.values(values)\n",
                "        keys = self.keys(keys)\n",
                "        queries = self.queries(queries)\n",
                "\n",
                "        # For multi-head attention, the V, K, Q vectors are split into N parts\n",
                "        # That is 8 parts\n",
                "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
                "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
                "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
                "\n",
                "        # Using einsum for flexibility and manual broadcasting\n",
                "        # It is performing Q.K^T (Q multiplied to the transpose of K)\n",
                "        # Here n: batch size; q, k: query, key length; h: heads; d: head_dim\n",
                "        # It says multiply q & k matching n & h, summing over d\n",
                "        attention_scores = torch.einsum(\"nqhd,nkhd->nhqk\", queries, keys)\n",
                "\n",
                "        # For making the model causal\n",
                "        if mask is not None:\n",
                "            mask = mask.bool()\n",
                "            attention_scores = attention_scores.masked_fill(~mask, float(\"-1e20\"))\n",
                "\n",
                "        # Here it is basically doing Softmax((Q.K^T)/sqrt(d_k))\n",
                "        # Which is head_dim wrt K, but here all Q, K, V has same head_sim\n",
                "        attention = torch.softmax(attention_scores / (self.head_dim ** 0.5), dim=-1)\n",
                "        attention = self.attn_dropout(attention)\n",
                "\n",
                "        # Here n: batch size; h: heads; q,v: query, value length; d: head_dim\n",
                "        # It says multiply attention and values matching n & h, summing over v\n",
                "        # This is basically doing Softmax((Q.K^T)/sqrt(d_k))*V\n",
                "        out = torch.einsum(\"nhqv,nvhd->nqhd\", attention, values)\n",
                "\n",
                "        # Previously for multi-head attention, it was split into N parts\n",
                "        # Now all those N parts are being concatenated\n",
                "        out = out.reshape(N, query_len, self.embed_size)\n",
                "\n",
                "        # This is a layer to average the information from all heads(W^0 from the paper)\n",
                "        return self.fc_out(out)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Transformer Block\n",
                "The `TransformerBlock` serves as the fundamental building block of the Encoder. It consists of the self-attention layer followed by a feed-forward network (MLP), with residual connections and layer normalization applied after each sub-layer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TransformerBlock(nn.Module):\n",
                "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
                "        # Here, attention is the SelfAttention class\n",
                "        # norm1 & norm2 are Layer Normalization methods for Add & Norm\n",
                "        super(TransformerBlock, self).__init__()\n",
                "        self.attention = SelfAttention(embed_size, heads, dropout)\n",
                "        self.norm1 = nn.LayerNorm(embed_size)\n",
                "        self.norm2 = nn.LayerNorm(embed_size)\n",
                "\n",
                "        # In paper, for position-wise FFN, the inner-layer has a\n",
                "        # dimensionality d_ff = 2048, which is 4*d_model(embed_size)\n",
                "        # And a ReLU is used in between\n",
                "        self.feed_forward = nn.Sequential(\n",
                "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
                "        )\n",
                "\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, value, key, query, mask):\n",
                "        attention = self.attention(value, key, query, mask)\n",
                "        # Implements the residual connection b/w attention & Q\n",
                "        x = self.dropout(self.norm1(attention + query))\n",
                "        forward = self.feed_forward(x)\n",
                "        # Implements the second residual connection b/w the\n",
                "        # output of 1st Add & Norm sub-layer and output of FFN       \n",
                "        out = self.dropout(self.norm2(forward + x))\n",
                "        return out"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Encoder\n",
                "The `Encoder` is composed of a stack of `TransformerBlock` layers. It takes the source sequence, adds positional embeddings to the word embeddings, and passes the result through the layers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Encoder(nn.Module):\n",
                "    def __init__(self, src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length):\n",
                "        super(Encoder, self).__init__()\n",
                "        # Here the device is assigned\n",
                "        # word_embedding stores the embedding of src_vocab_size\n",
                "        # & each emdedding is of embed_size (d_model) size\n",
                "        # Similarly positional_embedding stores embedding of max_length\n",
                "        self.device = device\n",
                "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
                "        self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
                "\n",
                "        # For implementing num_layers(Nx) no. of TransformerBlock as per the paper\n",
                "        self.layers = nn.ModuleList(\n",
                "            [\n",
                "                TransformerBlock(\n",
                "                    embed_size, heads, dropout, forward_expansion\n",
                "                )\n",
                "                for _ in range(num_layers)\n",
                "            ]\n",
                "        )\n",
                "\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x, mask):\n",
                "        N, seq_length = x.shape\n",
                "        # Creates the positional indices from 0 to seq_len-1\n",
                "        # & then copies the same vector N times, \n",
                "        # so that each batch gets its own positional indices\n",
                "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
                "\n",
                "        # This creates the entry point into the encoder by adding \n",
                "        # input embedding with positional encoding created by the position indices\n",
                "        # IMPORTANT-> Although the original paper used sin & cos to make positional embeddings\n",
                "        # but they specifically stated that using learned positional embeddings produces\n",
                "        # nearly identical result, so we are relying on nn.Embedding method\n",
                "        out = self.dropout(self.word_embedding(x) + self.positional_embedding(positions))\n",
                "\n",
                "        # Pass the data through N layers\n",
                "        for layer in self.layers:\n",
                "            out = layer(out, out, out, mask)\n",
                "\n",
                "        return out"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Decoder Block\n",
                "The `DecoderBlock` is similar to the Transformer Block but includes a masked self-attention layer (to prevent attending to future tokens during training) and a cross-attention layer that attends to the encoder output (keys and values from encoder, queries from decoder)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DecoderBlock(nn.Module):\n",
                "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
                "        # Assign SelfAttention, LayerNorm, TransformerBlock\n",
                "        super(DecoderBlock, self).__init__()\n",
                "        self.attention = SelfAttention(embed_size, heads, dropout)\n",
                "        self.norm = nn.LayerNorm(embed_size)\n",
                "        # Reusing the TransformerBlock class used in encoder\n",
                "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x, value, key, src_mask, tgt_mask):\n",
                "        # Implements masked self-attention\n",
                "        # tgt_mask makes the model causal\n",
                "        attention = self.attention(x, x, x, tgt_mask)\n",
                "        # It implements a residual connection between attention & x\n",
                "        query = self.dropout(self.norm(attention + x))\n",
                "        # Implements cross-attention\n",
                "        # Here value & key comes from encoder, but query comes from decoder\n",
                "        out = self.transformer_block(value, key, query, src_mask)\n",
                "        return out"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Decoder\n",
                "The `Decoder` consists of a stack of `DecoderBlock` layers. It processes the target sequence (shifted right) and produces logits for the next token prediction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Decoder(nn.Module):\n",
                "    def __init__(self, tgt_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length):\n",
                "        # Here the device is assigned\n",
                "        # word_embedding stores the embedding of tgt_vocab_size\n",
                "        # & each emdedding is of embed_size (d_model) size\n",
                "        # Similarly positional_embedding stores embedding of max_length\n",
                "        super(Decoder, self).__init__()\n",
                "        self.device = device\n",
                "        self.word_embedding = nn.Embedding(tgt_vocab_size, embed_size)\n",
                "        self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
                "\n",
                "         # For implementing num_layers(Nx) no. of DecoderBlock as per the paper\n",
                "        self.layers = nn.ModuleList(\n",
                "            [\n",
                "                DecoderBlock(embed_size, heads, forward_expansion, dropout)\n",
                "                for _ in range(num_layers)\n",
                "            ]\n",
                "        )\n",
                "\n",
                "        # This is the top layer\n",
                "        # It projects the final vector size to tgt_vocab_size\n",
                "        self.fc_out = nn.Linear(embed_size, tgt_vocab_size)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x, enc_out, src_mask, tgt_mask):\n",
                "        N, seq_length = x.shape\n",
                "        # Creates the positional indices from 0 to seq_len-1\n",
                "        # & then copies the same vector N times, \n",
                "        # so that each batch gets its own positional indices\n",
                "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
                "\n",
                "        # This creates the entry point into the encoder by adding \n",
                "        # input embedding with positional encoding created by the position indices\n",
                "        x = self.dropout(self.word_embedding(x) + self.positional_embedding(positions))\n",
                "\n",
                "        # Pass the data through N layers\n",
                "        for layer in self.layers:\n",
                "            x = layer(x, enc_out, enc_out, src_mask, tgt_mask)\n",
                "\n",
                "        # Returns the logits\n",
                "        return self.fc_out(x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Transformer\n",
                "The full `Transformer` model combines the Encoder and Decoder. It also generates the necessary masks for padding and causal attention."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Transformer(nn.Module):\n",
                "    def __init__(self, src_vocab_size, tgt_vocab_size, src_pad_idx, tgt_pad_idx, embed_size=512, num_layers=6, forward_expansion=4,\n",
                "        heads=8, dropout=0.1, device=\"cpu\", max_length=100):\n",
                "        # Initializes the encoder & decoder along with src & tgt pad_idx & device\n",
                "        super(Transformer, self).__init__()\n",
                "        self.encoder = Encoder(src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length)\n",
                "        self.decoder = Decoder(tgt_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length)\n",
                "        self.src_pad_idx = src_pad_idx\n",
                "        self.tgt_pad_idx = tgt_pad_idx\n",
                "        self.device = device\n",
                "\n",
                "    def make_src_mask(self, src):\n",
                "        # Adds extra dimension at 1 & 2 dim\n",
                "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
                "        return src_mask.to(self.device)\n",
                "\n",
                "    def make_tgt_mask(self, tgt):\n",
                "        N, tgt_len = tgt.shape\n",
                "        tgt_pad_mask = (tgt != self.tgt_pad_idx).unsqueeze(1).unsqueeze(2)\n",
                "        # Adds a matrix of 1s in lower triangular part\n",
                "        causal_mask = torch.tril(torch.ones((tgt_len, tgt_len))).bool().to(self.device)\n",
                "        return tgt_pad_mask & causal_mask\n",
                "\n",
                "    def forward(self, src, tgt):\n",
                "        src_mask = self.make_src_mask(src)\n",
                "        tgt_mask = self.make_tgt_mask(tgt)\n",
                "        enc_out = self.encoder(src, src_mask)\n",
                "        return self.decoder(tgt, enc_out, src_mask, tgt_mask)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Training & Data Loading\n",
                "Here we load the English-Bengali dataset, build the vocabulary, and train the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running on: cuda\n",
                        "Epoch 50: Loss = 1.6909\n",
                        "Epoch 100: Loss = 0.5681\n",
                        "Epoch 150: Loss = 0.2540\n",
                        "Epoch 200: Loss = 0.1300\n",
                        "Epoch 250: Loss = 0.0771\n",
                        "Epoch 300: Loss = 0.0443\n",
                        "Epoch 350: Loss = 0.0342\n",
                        "Epoch 400: Loss = 0.0267\n",
                        "Epoch 450: Loss = 0.0186\n",
                        "Epoch 500: Loss = 0.0167\n"
                    ]
                }
            ],
            "source": [
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Running on: {device}\")\n",
                "\n",
                "CSV_FILE = 'data.csv'\n",
                "csv_data = []\n",
                "with open(CSV_FILE, mode='r', newline='', encoding='utf-8') as f:\n",
                "    reader = csv.reader(f)\n",
                "    for row in reader:\n",
                "        csv_data.append(row)\n",
                "\n",
                "class Vocabulary:\n",
                "    def __init__(self):\n",
                "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
                "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.itos)\n",
                "\n",
                "        # Here it loops through every word in every sentence \n",
                "        # and adds an unique index starting from 4 to every other words\n",
                "        # other than those 4 tokens: <PAD>, <SOS>, <EOS>, <UNK>\n",
                "    def build_vocab(self, sentences):\n",
                "        idx = 4\n",
                "        for sentence in sentences:\n",
                "            for word in sentence.lower().split():\n",
                "                if word not in self.stoi:\n",
                "                    self.stoi[word] = idx\n",
                "                    self.itos[idx] = word\n",
                "                    idx += 1\n",
                "    \n",
                "        # Here it converts the text into a list of indices and if \n",
                "        # the word is not in the vocabulary it returns <UNK>\n",
                "    def numericalize(self, text):\n",
                "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in text.lower().split()]\n",
                "\n",
                "# Read from 2nd line because 1st line can be a header\n",
                "eng_sentences = [row[0] for row in csv_data[1:]]\n",
                "ben_sentences = [row[1] for row in csv_data[1:]]\n",
                "\n",
                "# Build the vocabulary\n",
                "src_vocab = Vocabulary()\n",
                "src_vocab.build_vocab(eng_sentences)\n",
                "tgt_vocab = Vocabulary()\n",
                "tgt_vocab.build_vocab(ben_sentences)\n",
                "\n",
                "# Prepare src & tgt indices\n",
                "src_indices = []\n",
                "tgt_indices = []\n",
                "\n",
                "# Here it adds <SOS> at the start and <EOS> at the end of every sentence\n",
                "for src_text, tgt_text in zip(eng_sentences, ben_sentences):\n",
                "    s_idx = [src_vocab.stoi[\"<SOS>\"]] + src_vocab.numericalize(src_text) + [src_vocab.stoi[\"<EOS>\"]]\n",
                "    t_idx = [tgt_vocab.stoi[\"<SOS>\"]] + tgt_vocab.numericalize(tgt_text) + [tgt_vocab.stoi[\"<EOS>\"]]\n",
                "    \n",
                "    src_indices.append(torch.tensor(s_idx))\n",
                "    tgt_indices.append(torch.tensor(t_idx))\n",
                "\n",
                "# Pad all sentences to match the longest one by adding <PAD>\n",
                "src_batch = pad_sequence(src_indices, padding_value=src_vocab.stoi[\"<PAD>\"], batch_first=True).to(device)\n",
                "tgt_batch = pad_sequence(tgt_indices, padding_value=tgt_vocab.stoi[\"<PAD>\"], batch_first=True).to(device)\n",
                "\n",
                "# Transformer setup\n",
                "model = Transformer(src_vocab_size=len(src_vocab), tgt_vocab_size=len(tgt_vocab), src_pad_idx=src_vocab.stoi[\"<PAD>\"], tgt_pad_idx=tgt_vocab.stoi[\"<PAD>\"],\n",
                "    embed_size=64, num_layers=2, forward_expansion=2, heads=4, dropout=0.1, device=device, max_length=100).to(device)\n",
                "\n",
                "\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.stoi[\"<PAD>\"])\n",
                "\n",
                "# Training \n",
                "model.train()\n",
                "\n",
                "for epoch in range(500):\n",
                "    optimizer.zero_grad()\n",
                "    \n",
                "    # Implementation of 'Teacher Forcing' in a Seq2Seq transformer\n",
                "    output = model(src_batch, tgt_batch[:, :-1])\n",
                "    \n",
                "    # Reshape\n",
                "    output = output.reshape(-1, len(tgt_vocab))\n",
                "    target = tgt_batch[:, 1:].reshape(-1)\n",
                "    \n",
                "    loss = criterion(output, target)\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "\n",
                "    if (epoch + 1) % 50 == 0:\n",
                "        print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Evaluation\n",
                "We test the model on a few sample sentences to see how well it generalizes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Testing->\n",
                        "\n",
                        "Input 1: We study machine learning\n",
                        "Output 1: আমরা মেশিন লার্নিং ব্যবহার করি\n",
                        "\n",
                        "Input 2: Arjun bought a new computer\n",
                        "Output 2: অর্জুন একটি নতুন কম্পিউটার কিনেছে\n",
                        "\n",
                        "Input 3: She drinks tea\n",
                        "Output 3: সে চা পান করে\n",
                        "\n",
                        "Input 4: she read machine learning\n",
                        "Output 4: সে মেশিন লার্নিং পড়ে\n",
                        "\n",
                        "Input 5: We play ludo\n",
                        "Output 5: আমরা লুডো খেলে\n",
                        "\n",
                        "Input 6: Rohan plays football\n",
                        "Output 6: রোহন ফুটবল খেলে\n",
                        "\n",
                        "Input 7: Puja eats rice\n",
                        "Output 7: পূজা ভাত খায়\n",
                        "\n",
                        "Input 8: We drink tea\n",
                        "Output 8: আমরা চা পান করে\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Testing\n",
                "print(\"Testing->\")\n",
                "print()\n",
                "\n",
                "model.eval()\n",
                "\n",
                "test_sentences = [\n",
                "    \"We study machine learning\",\n",
                "    \"Arjun bought a new computer\",\n",
                "    \"She drinks tea\",\n",
                "    \"she read machine learning\",\n",
                "    \"We play ludo\",\n",
                "    \"Rohan plays football\",\n",
                "    \"Puja eats rice\",\n",
                "    \"We drink tea\"\n",
                "]\n",
                "\n",
                "def translate_sentence(sentence, model, src_vocab, tgt_vocab, device, max_len=20):\n",
                "    # Tokenize and add <SOS>/<EOS>\n",
                "    src_idx = [src_vocab.stoi[\"<SOS>\"]] + src_vocab.numericalize(sentence) + [src_vocab.stoi[\"<EOS>\"]]\n",
                "    src_tensor = torch.LongTensor(src_idx).unsqueeze(0).to(device) # Shape: (1, seq_len)\n",
                "    \n",
                "    # Start with <SOS>\n",
                "    outputs = [tgt_vocab.stoi[\"<SOS>\"]]\n",
                "    \n",
                "    for _ in range(max_len):\n",
                "        trg_tensor = torch.LongTensor([outputs]).to(device)\n",
                "\n",
                "        with torch.no_grad():\n",
                "            output = model(src_tensor, trg_tensor)\n",
                "\n",
                "        # Get the token with highest probability from the last step\n",
                "        best_guess = output[:, -1, :].argmax(1).item()\n",
                "\n",
                "        # Stop if model predicts EOS\n",
                "        if best_guess == tgt_vocab.stoi[\"<EOS>\"]:\n",
                "            break\n",
                "\n",
                "        outputs.append(best_guess)\n",
                "    \n",
                "    # Convert indices back to words (skipping <SOS>)\n",
                "    return \" \".join([tgt_vocab.itos[idx] for idx in outputs[1:]])\n",
                "\n",
                "for i, sentence in enumerate(test_sentences):\n",
                "    translation = translate_sentence(sentence, model, src_vocab, tgt_vocab, device)\n",
                "    print(f\"Input {i+1}: {sentence}\")\n",
                "    print(f\"Output {i+1}: {translation}\")\n",
                "    print()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
